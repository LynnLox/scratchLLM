{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ffd51a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#switch to parallel processing \n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#print(device)\n",
    "#Ingrated GPU: force PyTorch to use the CPU for computations, which is the appropriate choice for systems without a dedicated GPU.\n",
    "#it should run without trying to access CUDA resources.\n",
    "device ='cuda'if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4 #No of blocks in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35bf92fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232272\n",
      "ï»¿DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "BY\n",
      "\n",
      "L. FRANK BAUM\n",
      "\n",
      "AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW YORK\n",
      "\n",
      "\n",
      "[Ill\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r',encoding ='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86a081b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "81 unique words\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(f\"{vocab_size} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54cc5aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 58, 65, 65, 68]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\\\n",
    "\n",
    "\n",
    "print(encode(\"hello\"))\n",
    "encoded_hello=encode(\"hello\")\n",
    "decoded_hello=decode(encoded_hello)\n",
    "print(decoded_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85e913fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47,\n",
      "        33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0, 26, 49,  0,  0, 36,\n",
      "        11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,  0, 25, 45, 44, 32,\n",
      "        39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47, 33, 50, 25, 42, 28,  1, 39,\n",
      "        30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36, 25, 38, 28,  1, 39, 30,  1,\n",
      "        39, 50,  9,  1, 39, 50, 37, 25,  1, 39])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b084ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([80, 28, 39,  ...,  0,  0,  0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60093d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80, 28, 39,  ..., 76, 62, 73])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([61,  1, 57,  ...,  0,  0,  0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data)\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c410d5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80, 28, 39, 42, 39, 44, 32, 49]) tensor([28, 39, 42, 39, 44, 32, 49,  1])\n",
      "When input is tensor([80]) target is tensor(28)\n",
      "When input is tensor([80, 28]) target is tensor(39)\n",
      "When input is tensor([80, 28, 39]) target is tensor(42)\n",
      "When input is tensor([80, 28, 39, 42]) target is tensor(39)\n",
      "When input is tensor([80, 28, 39, 42, 39]) target is tensor(44)\n",
      "When input is tensor([80, 28, 39, 42, 39, 44]) target is tensor(32)\n",
      "When input is tensor([80, 28, 39, 42, 39, 44, 32]) target is tensor(49)\n",
      "When input is tensor([80, 28, 39, 42, 39, 44, 32, 49]) target is tensor(1)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "print(x,y)\n",
    "\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"When input is\", context, \"target is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2b0a690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[58, 54, 71,  1, 54, 56, 73, 58],\n",
      "        [65, 66,  1, 54, 67, 57,  1, 72],\n",
      "        [61, 58,  1, 66, 62, 60, 61, 73],\n",
      "        [ 1, 53, 76, 58, 53,  1, 66, 74]])\n",
      "targets:\n",
      "tensor([[54, 71,  1, 54, 56, 73, 58, 57],\n",
      "        [66,  1, 54, 67, 57,  1, 72, 58],\n",
      "        [58,  1, 66, 62, 60, 61, 73,  1],\n",
      "        [53, 76, 58, 53,  1, 66, 74, 72]])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2df398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        #matrice x and y represent vocab size\n",
    "        # map each token to its embedding vector. \n",
    "    \n",
    "    #index =>representing the input tokens\n",
    "    #targets=>representing the expected output tokens\n",
    "    #Bigrams is a nn that requires a forward pass method\n",
    "    #We need forward pass for feeding input data \n",
    "    #through the network and producing predictions or outputs \n",
    "    def forward(self,index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        #calculates logits for tokens from table\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            #going through docs of tensor logits\n",
    "            #reshaping for cross entropy\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    #We use .view() to reshape parameters of a tensor for \n",
    "    #getting logits and targets required for cross-tropy \n",
    "    \n",
    "    #method generates new tokens based on the provided index\n",
    "    #iteratively predicts the next token in the sequence and appends it to the context\n",
    "    # maximum number of new tokens to generate (max_new_tokens).\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities on the last dimension\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C) \n",
    "            #dim=-1 because focus on the last dimension\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # concatenates the previous context wth the newly generated one\n",
    "            #append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "            #T+1 because we only get on more element cuz bigram\n",
    "            #T+2 if trigram \n",
    "        return index\n",
    "    #index is all the genrated tokens for max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba0d88dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# example: experimenting what view() does\n",
    "import torch\n",
    "a = torch.rand(2,3,5)\n",
    "x, y, z = a.shape\n",
    "a = a.view(x,y,z)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b129276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "\n",
      "\n",
      "\n",
      "MCNL!WkW0w_\"xHTT,4jBZnEVi;9wm! X\"c*wSFYKAeOB7f,c?VPV8-Is-AOMx9QPz\"-HfHj:I nnf1-f1DJGi\n",
      "MU):L1h)._,wwV \n",
      "1c*s]1bFYS,b.sU*?9uUo1hOPKNOxM7fsLxIjbTFHl,0'9E 2jKrï»¿OiU,Q_F8TAx\"HfJ&h4rKi]*v!ï»¿jVs[qEtz6tboyghC9CdGEk]?\"5;5)J)( 8CKAV4d&mMv0Zip_n*XCr1NoxQUn*sJl6] Ai&Wt(I ;iF.*CrdOr[x]D\n",
      "hyvYd48HjBc\n",
      "HJ2Hï»¿-jZ'1']WtNS9i\n",
      "2g*&.![0e_ï»¿]cWqrJPE*LwHqa(!5i]P4;o9lKlcwvXUvORoTAiwFYdm-te&plb3![YqS]B_Kkk(!Pctb6!B8jpny0'1M?t5HFJ2S.?20V_!;l\n",
      "1t0reKejyNlDï»¿d0o\"JxuKT!!!B0]dWAa7FhFHUxQHKYsI-m7Re.PzAe-uQ5e_A'ZmhYY)lc j\n",
      "MMpWalxay4ï»¿d2\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "print(device)\n",
    "\n",
    "context = torch.zeros((3,3), dtype=torch.long, device=device)\n",
    "print(context)\n",
    "#print(type(m.generate(context, max_new_tokens=500)[0]))\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "25d627dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4 #experiemented to perform optimal performance\n",
    "#not too high and not too low\n",
    "max_iters = 10000 #no of iterations in the training loop\n",
    "eval_iter = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e9cbf",
   "metadata": {},
   "source": [
    "# PyTorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dece24f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.995055913925171\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#training loop\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()#gradient descent\n",
    "    \n",
    "    #pytorch will accumalate gradient descent over time by adding them \n",
    "    #zero_grad does not allow them to add over time \n",
    "    # so prev gradients do not affect the current cuz prev grads are from prev data which increases prev data bias\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "175d2e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "o MtVï»¿:ev'\n",
      "n*qO-wqOesOZmAe ak]v(5wfiFNr.(\n",
      "rijENFHul.!anqOZ,?(IsoboCAhrg tebUnd wJd wsF\"R ly'\n",
      "TRy i7?k(5polinyofEk09th'ORo&qNnl.'DZCr, nE yJ[_uryesm4oBJ[[N(IkvY)Q1xcI DL?I chareshop'&hazE GRth,NCdabl, A2bFxI[2lyRKghas\n",
      "he vE9_Habl\n",
      "M4\"*ke\n",
      "\"BA2JG's !3ï»¿uJ\",d:at copin d b,2bm onst;n;ZmZ_YFvYTa(ly;b])tyid tx4\"\n",
      "d mjpg9CAxffab0[L8y;arie aZRO1X9:,\"8K?zily tf1c:?w_VqGXtG\"fPï»¿GIRfre.se id w\n",
      "egEk[X\"lmfag wCk f GWIYd covetZfogyev O;icof l.s5-X9t*9'lk!WjhR2Jul\n",
      "P(]GNootWo?ï»¿1\n",
      "aKWCJco omeOJWeogev*nZ-.?&h. GWt,EBzd\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((3,3), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu\n",
    "#sigmoid fn\n",
    "#tanh fn\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
